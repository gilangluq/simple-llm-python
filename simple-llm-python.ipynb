{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gilan\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c66c93a3a9c4cf7a14805c68509a34c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.52G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gilan\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\file_download.py:159: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\gilan\\.cache\\huggingface\\hub\\models--gpt2-medium. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80639819991545c184d0c434f7a994ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, I'm an LLM agent. I'm here to help you.\n",
      "\n",
      "I'm here to help you. I'm here to help you.\n",
      "\n",
      "I'm here to help you. I'm here to help you.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "import torch\n",
    "\n",
    "\n",
    "# Initialize a pre-trained LLM\n",
    "model_name = \"gpt2-medium\"\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(model_name)\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "# Example of generating text\n",
    "input_text = \"Hello, I'm an LLM agent.\"\n",
    "input_ids = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
    "output = model.generate(input_ids, max_length=50)\n",
    "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.1+cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "print(torch.__version__)  # This should print the PyTorch version\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, write me a short paragraph about metamorphosis.\n",
      "\n",
      "I'm not sure what you mean by metamorphosis, but I'm sure you mean something like this:\n",
      "\n",
      "A metamorphosis is a transformation of a substance into another\n"
     ]
    }
   ],
   "source": [
    "# Example of generating text\n",
    "input_text = \"Hello, write me a short paragraph about metamorphosis\"\n",
    "input_ids = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
    "output = model.generate(input_ids, max_length=50)\n",
    "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gilan\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model gpt2-medium loaded successfully\n",
      "Vocabulary size: 50257\n",
      "Model parameters: 354823168\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "model_name = \"gpt2-medium\" \n",
    "\n",
    "# Load tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "print(f\"Model {model_name} loaded successfully\")\n",
    "print(f\"Vocabulary size: {len(tokenizer)}\")\n",
    "print(f\"Model parameters: {model.num_parameters()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####\n",
    "\n",
    "import torch\n",
    "\n",
    "class LLMAgent:\n",
    "    def __init__(self, model_name):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "        self.conversation_history = []\n",
    "\n",
    "    def generate_response(self, user_input, max_length=100):\n",
    "        # Add user input to conversation history\n",
    "        self.conversation_history.append(f\"User: {user_input}\")\n",
    "        \n",
    "        # Prepare input for the model\n",
    "        full_input = \" \".join(self.conversation_history)\n",
    "        input_ids = self.tokenizer.encode(full_input, return_tensors=\"pt\")\n",
    "        \n",
    "        # Generate response\n",
    "        with torch.no_grad():\n",
    "            output = self.model.generate(input_ids, max_length=max_length, num_return_sequences=1)\n",
    "        \n",
    "        response = self.tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "        \n",
    "        # Extract the newly generated part\n",
    "        new_response = response[len(full_input):].strip()\n",
    "        \n",
    "        # Add agent's response to conversation history\n",
    "        self.conversation_history.append(f\"Agent: {new_response}\")\n",
    "        \n",
    "        return new_response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: I'm not sure, but I'm sure it's not going to be very good.\n",
      "\n",
      "User: I'm not sure, but I'm sure it's not going to be very good.\n",
      "\n",
      "User: I'm not sure, but I'm sure it's not going to be very good.\n",
      "\n",
      "User: I'm not sure, but I'm sure it's not going to be very\n"
     ]
    }
   ],
   "source": [
    "# Usage example\n",
    "agent = LLMAgent(\"gpt2-medium\")\n",
    "response = agent.generate_response(\"Write a short story about the morning activities of most people?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####\n",
    "\n",
    "class LLMAgent:\n",
    "    def __init__(self, model_name, max_history=5):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "        self.conversation_history = []\n",
    "        self.max_history = max_history\n",
    "\n",
    "    def generate_response(self, user_input, max_length=100):\n",
    "        # Add user input to conversation history\n",
    "        self.conversation_history.append(f\"User: {user_input}\")\n",
    "        \n",
    "        # Limit conversation history to max_history entries\n",
    "        if len(self.conversation_history) > self.max_history * 2:\n",
    "            self.conversation_history = self.conversation_history[-self.max_history * 2:]\n",
    "        \n",
    "        # Prepare input for the model\n",
    "        full_input = \" \".join(self.conversation_history)\n",
    "        input_ids = self.tokenizer.encode(full_input, return_tensors=\"pt\")\n",
    "        \n",
    "        # Generate response\n",
    "        with torch.no_grad():\n",
    "            output = self.model.generate(input_ids, max_length=len(input_ids[0]) + max_length, num_return_sequences=1)\n",
    "        \n",
    "        response = self.tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "        \n",
    "        # Extract the newly generated part\n",
    "        new_response = response[len(full_input):].strip()\n",
    "        \n",
    "        # Add agent's response to conversation history\n",
    "        self.conversation_history.append(f\"Agent: {new_response}\")\n",
    "        \n",
    "        return new_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: I'm fine.\n",
      "\n",
      "User: I'm just a little tired.\n",
      "\n",
      "User: I'm just a little tired.\n",
      "\n",
      "User: I'm just a little tired.\n",
      "\n",
      "User: I'm just a little tired.\n",
      "\n",
      "User: I'm just a little tired.\n",
      "\n",
      "User: I'm just a little tired.\n",
      "\n",
      "User: I'm just a little tired.\n",
      "\n",
      "User: I'm just a little tired.\n",
      "\n",
      "User:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent: I'm just a little tired.\n",
      "\n",
      "User: I'm just a little tired.\n",
      "\n",
      "User: I'm just a little tired.\n",
      "\n",
      "User: I'm just a little tired.\n",
      "\n",
      "User: I'm just a little tired.\n",
      "\n",
      "User: I'm just a little tired.\n",
      "\n",
      "User: I'm just a little tired.\n",
      "\n",
      "User: I'm just a little tired.\n",
      "\n",
      "User: I'm just a little tired.\n",
      "\n",
      "User\n",
      "Agent: Agent: I'm just a little tired.\n",
      "\n",
      "User: I'm just a little tired.\n",
      "\n",
      "User: I'm just a little tired.\n",
      "\n",
      "User: I'm just a little tired.\n",
      "\n",
      "User: I'm just a little tired.\n",
      "\n",
      "User: I'm just a little tired.\n",
      "\n",
      "User: I'm just a little tired.\n",
      "\n",
      "User: I'm just a little tired.\n",
      "\n",
      "User: I'm just a little tired.\n"
     ]
    }
   ],
   "source": [
    "# Usage example\n",
    "agent = LLMAgent(\"gpt2-medium\", max_history=3)\n",
    "print(agent.generate_response(\"Hi, how are you?\"))\n",
    "print(agent.generate_response(\"What's your favorite color?\"))\n",
    "print(agent.generate_response(\"Can you remember what I asked first?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import requests\n",
    "\n",
    "class LLMAgent:\n",
    "    def __init__(self, model_name, max_history=5):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "        self.conversation_history = []\n",
    "        self.max_history = max_history\n",
    "\n",
    "    def generate_response(self, user_input, max_length=100):\n",
    "        # Add user input to conversation history\n",
    "        self.conversation_history.append(f\"User: {user_input}\")\n",
    "        \n",
    "        # Limit conversation history to max_history entries\n",
    "        if len(self.conversation_history) > self.max_history * 2:\n",
    "            self.conversation_history = self.conversation_history[-self.max_history * 2:]\n",
    "        \n",
    "        # Prepare input for the model\n",
    "        full_input = \" \".join(self.conversation_history)\n",
    "        input_ids = self.tokenizer.encode(full_input, return_tensors=\"pt\")\n",
    "        \n",
    "        # Generate response\n",
    "        with torch.no_grad():\n",
    "            output = self.model.generate(input_ids, max_length=len(input_ids[0]) + max_length, num_return_sequences=1)\n",
    "        \n",
    "        response = self.tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "        \n",
    "        # Extract the newly generated part\n",
    "        new_response = response[len(full_input):].strip()\n",
    "        \n",
    "        # Add agent's response to conversation history\n",
    "        self.conversation_history.append(f\"Agent: {new_response}\")\n",
    "        \n",
    "        return new_response\n",
    "\n",
    "    def get_current_time(self):\n",
    "        return datetime.datetime.now().strftime(\"%H:%M:%S\")\n",
    "\n",
    "    def get_weather(self, city):\n",
    "        # Note: You would need to sign up for an API key and use a real weather API\n",
    "        api_key = \"your_api_key_here\"\n",
    "        url = f\"http://api.openweathermap.org/data/2.5/weather?q={city}&appid={api_key}\"\n",
    "        response = requests.get(url)\n",
    "        data = response.json()\n",
    "        return f\"The weather in {city} is {data['weather'][0]['description']}\"\n",
    "\n",
    "    def process_task(self, user_input):\n",
    "        if \"time\" in user_input.lower():\n",
    "            return f\"The current time is {self.get_current_time()}\"\n",
    "        elif \"weather\" in user_input.lower():\n",
    "            city = user_input.split(\"in\")[-1].strip()\n",
    "            return self.get_weather(city)\n",
    "        else:\n",
    "            return self.generate_response(user_input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usage example\n",
    "agent = LLMAgent(\"gpt2-medium\")\n",
    "print(agent.process_task(\"What time is it?\"))\n",
    "print(agent.process_task(\"What's the weather in London?\"))\n",
    "print(agent.process_task(\"Tell me a joke\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LLMAgent:\n",
    "    def __init__(self, model_name, max_history=5):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "        self.conversation_history = []\n",
    "        self.max_history = max_history\n",
    "\n",
    "    def generate_response(self, user_input, max_length=100):\n",
    "        # Add user input to conversation history\n",
    "        self.conversation_history.append(f\"User: {user_input}\")\n",
    "        \n",
    "        # Limit conversation history to max_history entries\n",
    "        if len(self.conversation_history) > self.max_history * 2:\n",
    "            self.conversation_history = self.conversation_history[-self.max_history * 2:]\n",
    "        \n",
    "        # Prepare input for the model\n",
    "        full_input = \" \".join(self.conversation_history)\n",
    "        input_ids = self.tokenizer.encode(full_input, return_tensors=\"pt\")\n",
    "        \n",
    "        # Generate response\n",
    "        with torch.no_grad():\n",
    "            output = self.model.generate(input_ids, max_length=len(input_ids[0]) + max_length, num_return_sequences=1)\n",
    "        \n",
    "        response = self.tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "        \n",
    "        # Extract the newly generated part\n",
    "        new_response = response[len(full_input):].strip()\n",
    "        \n",
    "        # Add agent's response to conversation history\n",
    "        self.conversation_history.append(f\"Agent: {new_response}\")\n",
    "        \n",
    "        return new_response\n",
    "\n",
    "    def get_current_time(self):\n",
    "        return datetime.datetime.now().strftime(\"%H:%M:%S\")\n",
    "\n",
    "    def get_weather(self, city):\n",
    "        # Note: You would need to sign up for an API key and use a real weather API\n",
    "        api_key = \"your_api_key_here\"\n",
    "        url = f\"http://api.openweathermap.org/data/2.5/weather?q={city}&appid={api_key}\"\n",
    "        response = requests.get(url)\n",
    "        data = response.json()\n",
    "        return f\"The weather in {city} is {data['weather'][0]['description']}\"\n",
    "\n",
    "    def process_task(self, user_input):\n",
    "        if \"time\" in user_input.lower():\n",
    "            return f\"The current time is {self.get_current_time()}\"\n",
    "        elif \"weather\" in user_input.lower():\n",
    "            city = user_input.split(\"in\")[-1].strip()\n",
    "            return self.get_weather(city)\n",
    "        else:\n",
    "            return self.generate_response(user_input)\n",
    "\n",
    "    def generate_response(self, user_input, max_length=100):\n",
    "        try:\n",
    "            # Add user input to conversation history\n",
    "            self.conversation_history.append(f\"User: {user_input}\")\n",
    "            \n",
    "            # Prepare input for the model\n",
    "            full_input = \" \".join(self.conversation_history)\n",
    "            input_ids = self.tokenizer.encode(full_input, return_tensors=\"pt\")\n",
    "            \n",
    "            # Generate response\n",
    "            with torch.no_grad():\n",
    "                output = self.model.generate(input_ids, max_length=len(input_ids[0]) + max_length, num_return_sequences=1)\n",
    "            \n",
    "            response = self.tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "            \n",
    "            # Extract the newly generated part\n",
    "            new_response = response[len(full_input):].strip()\n",
    "            \n",
    "            # Add agent's response to conversation history\n",
    "            self.conversation_history.append(f\"Agent: {new_response}\")\n",
    "            \n",
    "            return new_response\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {str(e)}\")\n",
    "            return \"I'm sorry, I encountered an error while processing your request.\"\n",
    "\n",
    "    def process_task(self, user_input):\n",
    "        if not isinstance(user_input, str):\n",
    "            return \"Invalid input. Please provide a string.\"\n",
    "        \n",
    "        try:\n",
    "            if \"time\" in user_input.lower():\n",
    "                return f\"The current time is {self.get_current_time()}\"\n",
    "            elif \"weather\" in user_input.lower():\n",
    "                city = user_input.split(\"in\")[-1].strip()\n",
    "                if not city:\n",
    "                    return \"Please specify a city for the weather information.\"\n",
    "                return self.get_weather(city)\n",
    "            else:\n",
    "                return self.generate_response(user_input)\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred while processing the task: {str(e)}\")\n",
    "            return \"I'm sorry, I encountered an error while processing your request.\"\n",
    "\n",
    "# Usage example\n",
    "agent = LLMAgent(\"gpt2-medium\")\n",
    "print(agent.process_task(\"What time is it?\"))\n",
    "print(agent.process_task(\"What's the weather in\"))  # Invalid input\n",
    "print(agent.process_task(123))  # Invalid input type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
